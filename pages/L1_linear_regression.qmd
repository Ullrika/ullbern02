---
title: "Lecture Linear regression and other alternatives"
subtitle: "BERN02"
author: "Ullrika Sahlin"
format: 
  html:
    embed-resources: true
    theme: cosmo
    toc: true
echo: false
---

```{r}
#| message: false
#| warning: false
library(readr)
library(tidyr)
library(ggplot2)
library(pROC)
library(splines)
```

# Literature

ISL: 3.1, 3.2, 3.3, 7.6, 7.3, 7.4

# Parametric approach to linear regression

## Ordinary Least Squares (OLS) regression

1.  We select a linear model

$$f(x)=\beta_0+\beta_1 x_1 + \ldots + \beta_p x_p$$ 2. We estimate the parameters as those minimising the RSS.

Let $\beta=(\beta_0,\ldots,\beta_p)$ and

$$Q(\beta):= Q(\beta_0,\ldots,\beta_p) = \sum_{i=1}^{n} \left(y_i - (\beta_0+\beta_1 x_{i1} + \ldots + \beta_p x_{ip})\right)^2$$ then

$$\hat{\beta} = \underset{\beta}{\mathrm{argmin}} \ Q(\beta)$$

::: callout-note
Simple regression - a model with one independent variable, i.e. $p=1$

$$\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2}$$

$$\hat{\beta}_0 = \bar{y}-\hat{\beta}_1\bar{x}$$

Multiple regression - a model with several independent variables, i.e. $p>1$

> Parameters estimated by matrix calculations, not shown here

:::


## Airpollution data set

Here we illustrate regression with an air pollution data set from a paper on ridge regression. Source: McDonald, G.C. and Schwing, R.C. (1973) 'Instabilities of regression estimates relating air pollution to mortality', Technometrics, vol.15, 463-482.

[https://lib.stat.cmu.edu/datasets/pollution](https://lib.stat.cmu.edu/datasets/pollution)

Variables in order:
PREC   Average annual precipitation in inches
JANT   Average January temperature in degrees F
JULT   Same for July
OVR65  % of 1960 SMSA population aged 65 or older
POPN   Average household size
EDUC   Median school years completed by those over 22
HOUS   % of housing units which are sound & with all facilities
DENS   Population per sq. mile in urbanized areas, 1960
NONW   % non-white population in urbanized areas, 1960
WWDRK  % employed in white collar occupations
POOR   % of families with income < $3000
HC     Relative hydrocarbon pollution potential
NOX    Same for nitric oxides
SO2    Same for sulphur dioxide
HUMID  Annual average % relative humidity at 1pm
MORT   Total age-adjusted mortality rate per 100,000

```{r}
df <- read_csv("../data/pollution_cleaneddata.csv")
```

A simple linear regression on the association between proportion of poor families and mortality, together with a 95% confidence region for the line. 

```{r}
mod <- lm(MORT~POOR,df)
confint(mod)
newdata <- data.frame(POOR=seq(min(df$POOR),max(df$POOR),length.out=100))

pred <- predict(mod,newdata,interval="prediction")
df_pred <- cbind(newdata,pred)
```


```{r}
df %>%  ggplot(aes(x=POOR,y=MORT)) +
geom_point() +
ylab("MORT: \n Total age-adjusted mortality rate per 100 000") +
xlab("POOR: \n % of families with income < $3000") +
geom_smooth(method="lm",formula = 'y ~ x') +
geom_line(data=df_pred,aes(x=POOR,y=lwr),linetype="dashed") +
geom_line(data=df_pred,aes(x=POOR,y=upr),linetype="dashed") 
```



# Bias and precision 

We want estimates or predictions to be unbiased and have high precision (i.e. low variance). 

```{r}
#| echo: false
#| 
# code modified from https://www.datawim.com/post/art-with-r/
  
# target board function
target_board <- function(title) {
  plot(0, 0, cex = 45, pch = 21, bg = "orange", col = "black", axes=F, main = title, xlab = NA, ylab = NA)
  points(0, 0, cex = 39, pch = 21, bg = "white", col = "white")
  points(0, 0, cex = 33, pch = 21, bg = "orange", col = "orange")
  points(0, 0, cex = 27, pch = 21, bg = "white", col = "white")
  points(0, 0, cex = 21, pch = 21, bg = "orange", col = "orange")
  points(0, 0, cex = 15, pch = 21, bg = "white", col = "white")
  points(0, 0, cex = 9, pch = 21, bg = "orange", col = "orange")
  points(0, 0, cex = 3, pch = 21, bg = "white", col = "white")
}

# 2 by 2 grid
par(mfrow = c(2, 2))

# remove extra white space
par(mar = c(1, 1, 1.5, 1)) 

# High Accuracy High Precision
s1 = 0.1
target_board(title = "High Accuracy High Precision")
points(rnorm(1,0,s1), rnorm(1,0,s1), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s1), rnorm(1,0,s1), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s1), rnorm(1,0,s1), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s1), rnorm(1,0,s1), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s1), rnorm(1,0,s1), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")

# Low Accuracy High Precision
m2 = 0.2
s2 = 0.1
target_board(title = "Low Accuracy High Precision")
points(rnorm(1,m2,s2), rnorm(1,m2,s2), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m2,s2), rnorm(1,m2,s2), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m2,s2), rnorm(1,m2,s2), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m2,s2), rnorm(1,m2,s2), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m2,s2), rnorm(1,m2,s2), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")

# High Accuracy Low Precision
s3 = 0.3
target_board(title = "High Accuracy Low Precision")
points(rnorm(1,0,s3), rnorm(1,0,s3), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s3), rnorm(1,0,s3), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s3), rnorm(1,0,s3), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s3), rnorm(1,0,s3), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,0,s3), rnorm(1,0,s3), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")

# Low Accuracy Low Precision
m4 = 0.2
s4 = 0.3
target_board(title = "Low Accuracy Low Precision")
points(rnorm(1,m4,s4), rnorm(1,m4,s4), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m4,s4), rnorm(1,m4,s4), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m4,s4), rnorm(1,m4,s4), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m4,s4), rnorm(1,m4,s4), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
points(rnorm(1,m4,s4), rnorm(1,m4,s4), pch = 4, lwd = 2, cex = 1.5, col = "#3366FF")
```



## Statistical errors in simple linear regression

Under the assumptions of independent and identically distributed model errors, $\varepsilon_i$, the variance of the  errors can be estimated as 

$$\hat{\sigma}^2 = \frac{\sum_{i=1}^n e_i^2}{n-2}$$
where $e_i=y_i-\beta_0-\beta_1x_{i1}$.

The variance of the slope is 

$$V(\hat{\beta}_1) = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2}$$
If the sample size $n$ is sufficiently large and using the estimate for the variance $\sigma^2$ we can derive a frequentist $(1-\alpha)$ two-sided confidence interval as  

$$I_{\beta_1}: \hat{\beta}_1 \pm \lambda_{\alpha}\cdot \sqrt{\hat{V}(\hat{\beta}_1)}$$

The value of the quantile $\lambda_{\alpha}$ is chosen based on what level of confidence that is asked for. In the book, they use 2 and refer to this as a 95%th confidence interval.  



|Confidence level|$\alpha$|$\lambda_{\alpha}$ |
|----------------|--------|-------------------|
| 99%            | 0.5%   |  2.58             |
| 95%            | 2.5%   |  1.96             |
| 90%            | 5%     | 1.64              |
| 80%            | 10%    | 1.28              |

: Quantiles from a normal distribution for a two-sided confidence interval



The variance for the intercept is 

$$V(\hat{\beta}_0) = \sigma^2\left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i-\bar{x})^2} \right]$$



The variance of the *expected value* of the response variable given that $x=x_0$ is


$$\begin{split}  V(\hat{\mu}(x_0)) = V(\hat{\beta}_0+\hat{\beta}_1x_0) = &  & \\ \sigma^2\left[ \frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum_{i=1}^n (x_i-\bar{x})^2} \right] \end{split}$$

A two-sided confidence interval for the expected value of the response given that $x=x_0$ is

$$\hat{\mu}(x_0) \pm \lambda_{\alpha}\cdot \sqrt{V(\hat{\mu}(x_0))}$$

The variance of a *predicted value* of the response variable given that $x=x_0$ is

$$V(\hat{y}(x_0)) = V(\beta_0+\beta_1x_0+\varepsilon) = \sigma^2\left[ \frac{1}{n} + \frac{(x_0-\bar{x})^2}{\sum_{i=1}^n (x_i-\bar{x})^2} +1\right] $$

A two-sided confidence interval for a predicted value of the response given that $x=x_0$ is

$$\hat{y}(x_0) \pm \lambda_{\alpha}\cdot \sqrt{\hat{V}(\hat{y}(x_0))}$$

> Make sure you understand the difference between variance in the estimate of an expected value and variance in a predition of a new observation. 

# Non-parametric approach to regression

1.  Seek an estimate of the unknown function $f$ that gets close to data points without being too rough or wiggly.

::: callout-note
Non-parametric approaches require more observations compared to parametric approaches
:::

## Local regression (LOESS)

Local regression is a non-parametric method that fit a local regression at each point (over a grid of points) in predictor space. 

1.  Select a value on the x-variable $x=x_0$

2.  Select the $k$ nearest points to $x_0$

3.  Assign a weight to each point $K_{i0}$

4.  Fit a weighted simple OLS regression by minimising

$$\sum_{i=1}^{n} K_{i0} (y_i-\beta_0-\beta_1x_{i})^2$$

5.  The fitted value at $x_0$ is $$\hat{y}|x_0 = \hat{f}(x_0) = \hat{\beta_0} + \hat{\beta_1}x_0$$

6.  Repeat for different values on $x_0$


![Figure 7.9 from ISL](../img/fig7_9.png)

```{r}
df %>%  ggplot(aes(x=POOR,y=MORT)) +
geom_point() +
ylab("Total age-adjusted mortality rate per 100 000") +
xlab("% of families with income < $3000") +
geom_smooth(method="loess", formula = y~x)
```


## Spline regression

Fit a line in each region of the predictor space defined by knots, requiring continuity of each knot.


```{r}
mod_splines <- lm(MORT ~ bs(POOR, df = 3), data = df)
pred_splines <- predict(mod_splines , newdata = newdata, se = T)
df_pred <- cbind(newdata,MORT=pred_splines$fit,se=pred_splines$se.fit)
```


```{r}
df %>%  ggplot(aes(x=POOR,y=MORT)) +
ylab("MORT: \n Total age-adjusted mortality rate per 100 000") +
xlab("POOR: \n % of families with income < $3000") + 
geom_line(data=df_pred, aes(x=POOR,y=MORT), colour="#3366FF") +
#geom_line(data=df_pred, aes(x=POOR,y=fit - 2 * se.fit), lty = "dashed") +
#geom_line(data=df_pred, aes(x=POOR,y=fit + 2 * se.fit), lty = "dashed") +
geom_ribbon(data=df_pred,aes(x=POOR,ymin=MORT - 2 * se,ymax=MORT + 2 * se),colour="gray",alpha=0.2) +
geom_point() 

```


# Study questions

(@) What is the objective function for estimating parameters in OLS regression?

(@) What assumptions are made about residuals in OLS regression?

(@) Describe a way to estimate the variance of model errors in OLS regression? 

(@) What is the difference between bias and precision of an estimate or prediction? 

(@) What assumptions are made to construct a frequentist confidence interval for a parameter in OLS regression? 

(@) What is the formula for a frequentist confidence interval for the expected value of a parameter in OLS regression?

(@) What is the objective function for estimating parameters in Local regression? 

(@) What is the main difference between regression with splines and local regression? 