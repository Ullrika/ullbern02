---
title: "Exercise 1 Linear Regression Discussion"
subtitle: "BERN02"
author: "Ullrika Sahlin"
format: 
  html:
    embed-resources: true
    theme: cosmo
echo: false
---


```{r}
#| message: false
#| warning: false
library(readr)
library(dplyr)
library(ggplot2)
library(ISLR2)
library(knitr)

```


## Load data

```{r}
#| message: false
#| warning: false

df <- read_csv("../data/pollution_cleaneddata.csv")
```

## Visualise data

```{r}
df %>%  ggplot(aes(x=POOR,y=MORT)) +
geom_point() +
ylab("Total age-adjusted mortality rate per 100 000") +
xlab("% of families with income < $3000") +
geom_smooth(method="loess", formula = y~x)
```


## What weight to choose?

The function for LOESS that comes with the ISL book use the tricube function to determine weights

$$Y = \left\{ \begin{array}{lr}
        1 & \text{if success}\\
        0 & \text{if failure}
        \end{array}\right.$$

$$W(u) = \left\{ \begin{array}{lr} 
1-(1-u^3)^3 & \text{for } 0<u<1 \\
0 & \text{otherwise} \end{array}\right.$$

where $u$ is the euclidean distance divided by the maximum distance among the locally selected points. 

## Predictions when using 10 nearest neighbors 

```{r}
k = 10
x0 = c(10,18,25)
fit <- loess(MORT ~ POOR, alpha=k/nrow(df), data = df, degree = 1)
pred <- predict(fit,newdata=data.frame(POOR = x0),se = TRUE)

kable(data.frame(x0=x0,y=pred$fit,se=pred$se.fit),format = "simple", digits = 2)

```

## Weighted OLS

The intercept and slope parameter is estimated by minimising the weighted sum of squares


## How to estimate prediction error from a Weighted OLS

Assume constant variance for the errors. Estimate the variance for the error $V(\varepsilon)$ from the residual sum of squares RSS, where we divide by the number of data points minus the number of estimated parameters, i.e. $k-2$

$$\frac{\sum_{i=1}^{k}(y_i-\hat{y}_i)^2}{k-2}$$


Repetition: We assume the errors have the same variance (homoscedastic), and can therefore estimate the variance of the error from the sum of squares (non-weighted)

$$\hat{\sigma}^2 = \frac{\sum_{i=1}^{k}(y_i-(\hat{\beta}_0+\hat{\beta}_1x_i))^2}{k-2}$$

The variance for the estimate of the expected value of the reponse given $x_0$ is 

$$V(\hat{\mu}(x_0)) = \sigma^2 \left( \frac{1}{k}  + \frac{(x_0-\bar{x})^2}{\sum_{i=1}^k (x_i-\bar{x})^2} \right)$$

where we plug in the estimated variance to get the standard error for the prediction $\hat{V}(\hat{\mu}(x_0))$.

## Linear regression with matrices  

A matrix specification of the linear regression model (can easily be expanded to $p$ predictors)

$$y = X\beta + e$$


$$Y = \left( \begin{array}{c}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n 
        \end{array}\right)$$
        
        
$$X = \left( \begin{array}{l,c}
        1 & x_1 \\
        1 & x_2 \\
        \vdots & \vdots \\
        1 & x_n 
        \end{array}\right)$$


$$\beta = \left( \begin{array}{c}
        \beta_0 \\
        \beta_1 \\
        \end{array}\right)$$

$$e = \left( \begin{array}{c}
        \varepsilon_1 \\
        \varepsilon_2 \\
        \vdots \\
        \varepsilon_n 
        \end{array}\right)$$
        
The OLS estimate of the parameters are 

$$\hat{\beta} = (X^TX)^{-1}X^Ty$$
The estimate of the variance is 

$$\hat{\sigma}^2 = \frac{y^Ty-\hat{\beta}^TX^Ty}{n-(p+1)}$$
A point prediction of $Y$ for $X_0=\left(1 \ x_0\right)$ (here we have only one predictor, $p = 1$) is 

$$\hat{\mu}(x_0) = X_0\hat{\beta}$$

The variance of the estimated expected value of $Y|x_0$ is 

$$V(\hat{\mu}(x_0)) = \sigma^2\left( X_0 (X^TX)^{-1}X_0^T \right)$$


```{r}
make_pred <- function(x0){
#x0 = 10
k = 10
dist <- abs(df$POOR-x0)
id <- sort.int(dist,index.return=TRUE)$ix[1:k]
u <- dist[id]/max(dist[id])
w <- (1-u^3)^3
w=w/sum(w)
wmod <- lm(df$MORT[id] ~ df$POOR[id], weights=w)
xmat <- model.matrix(wmod)
wmat <- diag(w)
y = df$MORT[id]
beta <- (solve(t(xmat)%*%wmat%*%xmat))%*%t(xmat)%*%wmat%*%y
sig2_hat <- sum((y-xmat%*%beta)^2/(k-2))
x0_mat = matrix(c(1,x0),ncol=2)
predy <- x0_mat%*%beta
mu_sd <- sqrt(sig2_hat*(x0_mat%*%(solve(t(xmat)%*%xmat))%*%t(x0_mat)))
#mu_sd <- sqrt(sig2_hat*(1/k + (x0-mean(df$POOR[id]))^2/sum((df$POOR[id]-mean(df$POOR[id]))^2)))
return(data.frame(x0=x0,y=predy,se=mu_sd))
}
```

```{r}
x0_val <- c(10,18,25)
sapply(x0_val,make_pred)
```









